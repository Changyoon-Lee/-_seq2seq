{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq(TENSOR).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMnXtNXz1hcnqxHo8XyEKsJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Changyoon-Lee/realization_seq2seq/blob/master/seq2seq(TENSOR).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNmKPHuqam1J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "outputId": "0ea869c0-da8f-4c7c-9d02-9f195bf868de"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 55.1MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/f7/a368401e630f0e390dd0e62c39fb928e5b23741b53c2360ee7d376660927/JPype1-1.0.2-cp36-cp36m-manylinux2010_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 50.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.9MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, tweepy, beautifulsoup4, colorama, konlpy\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.0.2 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv-nGWB3aeXs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "984d0d37-7f6e-4c38-e6b7-caf143209316"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5cNaBHybT0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "pf = pd.read_excel('/gdrive/My Drive/B반/data/kor.xlsx', sheet_name='Sheet1')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_x1nsUcjGDH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "171c0d61-6a96-45db-a8b1-9273a2e4e216"
      },
      "source": [
        "pf.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 75000 entries, 0 to 74999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   mid_sid  75000 non-null  int64 \n",
            " 1   ko       75000 non-null  object\n",
            " 2   en       75000 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 1.7+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXus8NdNb73Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentences_eng(sentences):\n",
        "    sentence = []\n",
        "    for line in sentences:\n",
        "        w = '<start> '+line+' <end>'\n",
        "        sentence.append(w)\n",
        "    return sentence\n",
        "def preprocess_sentences_kor(sentences):\n",
        "    okt = Okt()\n",
        "    sentence = []\n",
        "    for line in sentences:\n",
        "        tokens = [i[0] for i in okt.pos(line)]\n",
        "        tokens.insert(0,'<end>')\n",
        "        tokens.append('<start>')\n",
        "        sentence.append(' '.join(tokens[::-1]))\n",
        "    return sentence\n",
        "\n",
        "def backprocess_sentence(sentence):\n",
        "    tokens = sentence.split()\n",
        "    tokens = tokens[1:-1]\n",
        "    return ' '.join(tokens)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77RvT7izsKYu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "2965cf01-7fb1-4ec7-b2e2-c50362276021"
      },
      "source": [
        "pf['ko'][:5]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    나는 매일 저녁 배트를 만나러 다락방으로 가요.\n",
              "1            선생님 이문장이 이해가 안 가요.\n",
              "2      컴퓨터를 시작하면 시간이 너무 빠르게 가요.\n",
              "3         나는 오늘 자정에 한국으로 돌아 가요.\n",
              "4            나는 일어나자마자 화장실에 가요.\n",
              "Name: ko, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sioFo_BLsKde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='', oov_token='oov')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBaRNUxqgs81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_en, test_en, train_kor, test_kor = train_test_split(pf['en'], pf['ko'], test_size=0.2, random_state=123)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVHKapVIvVle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_en = preprocess_sentences_eng(train_en)\n",
        "train_kor = preprocess_sentences_kor(train_kor)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwoQt-NAuIEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_set 들로 tokenizer에 fit 시킨다\n",
        "input_tensor_train, inp_lang = tokenize(train_kor)\n",
        "target_tensor_train, targ_lang = tokenize(train_en)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIKfzXNfwChu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0066d1fe-0499-4c00-bc5b-a3190f2125f9"
      },
      "source": [
        "max_length_targ, max_length_inp = target_tensor_train.shape[1], input_tensor_train.shape[1]\n",
        "print(max_length_targ, max_length_inp)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18 26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQDndHCyh7Xm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_test(sent,lang='en'):\n",
        "    if lang=='ko':\n",
        "        tensor = inp_lang.texts_to_sequences(sent)\n",
        "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,maxlen = max_length_inp,\n",
        "                                                         padding='post')\n",
        "        return tensor\n",
        "    else :\n",
        "        tensor = targ_lang.texts_to_sequences(sent)\n",
        "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,maxlen =max_length_targ,\n",
        "                                                         padding='post')\n",
        "        return tensor"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q2EPlCJyQSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor_test = tokenize_test(test_kor, lang='ko')\n",
        "target_tensor_test = tokenize_test(test_en, lang='en')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK4C7z95ubiY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "a664e1c1-e4de-420d-d96c-68db7ad0fb17"
      },
      "source": [
        "input_tensor_test"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  1,   1, 882, ...,   0,   0,   0],\n",
              "       [  5,   1,   1, ...,   0,   0,   0],\n",
              "       [ 55,   1,   1, ...,   0,   0,   0],\n",
              "       ...,\n",
              "       [  5,   1,  56, ...,   0,   0,   0],\n",
              "       [  1, 291, 578, ...,   0,   0,   0],\n",
              "       [  1,   1, 202, ...,   0,   0,   0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoTj9yiTwUrh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "e58dae23-4c1c-468e-8e09-ed7a783f60a9"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 ----> <start>\n",
            "114 ----> korean\n",
            "447 ----> women\n",
            "16 ----> have\n",
            "326 ----> beautiful\n",
            "5975 ----> hairstyle\n",
            "12 ----> and\n",
            "1513 ----> color.\n",
            "3 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hAyyyRkxw9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_thX5ObOxxfY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0883248f-8b9d-4907-bab2-549b788fa50d"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape\n",
        " "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 26]), TensorShape([64, 18]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDWD3W71xxdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JlgiYGMzfXN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "ba20d863-a6bb-4060-d992-c374d7d89f4f"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 26, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM0sHV-1ze8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGu1Dh8Gzex-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "dc199ea8-7532-4bc2-b514-2d3eff1bc303"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 26, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_ubfXA-zjGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA6xP2Xpzjjk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a523a2f8-204d-4a6a-de58-ceed556196e3"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 28898)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo98Sj63zi-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oqP_veazp59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = '/gdrive/My Drive/강의자료'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIgGAjpgzpw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47tpoLZEzsjT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "da51c81a-fe99-451a-abf3-0e8bdcca9c4d"
      },
      "source": [
        "EPOCHS = 6\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 5.6886\n",
            "Epoch 1 Batch 100 Loss 3.6646\n",
            "Epoch 1 Batch 200 Loss 3.5089\n",
            "Epoch 1 Batch 300 Loss 3.3232\n",
            "Epoch 1 Batch 400 Loss 3.1465\n",
            "Epoch 1 Batch 500 Loss 3.0700\n",
            "Epoch 1 Batch 600 Loss 2.9208\n",
            "Epoch 1 Batch 700 Loss 2.9361\n",
            "Epoch 1 Batch 800 Loss 2.7497\n",
            "Epoch 1 Batch 900 Loss 2.8867\n",
            "Epoch 1 Loss 3.1811\n",
            "Time taken for 1 epoch 609.4964900016785 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.7591\n",
            "Epoch 2 Batch 100 Loss 2.4632\n",
            "Epoch 2 Batch 200 Loss 2.5409\n",
            "Epoch 2 Batch 300 Loss 2.6328\n",
            "Epoch 2 Batch 400 Loss 2.6400\n",
            "Epoch 2 Batch 500 Loss 2.6369\n",
            "Epoch 2 Batch 600 Loss 2.6490\n",
            "Epoch 2 Batch 700 Loss 2.4660\n",
            "Epoch 2 Batch 800 Loss 2.2688\n",
            "Epoch 2 Batch 900 Loss 2.4137\n",
            "Epoch 2 Loss 2.5870\n",
            "Time taken for 1 epoch 581.8003251552582 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 2.2290\n",
            "Epoch 3 Batch 100 Loss 2.1886\n",
            "Epoch 3 Batch 200 Loss 2.3801\n",
            "Epoch 3 Batch 300 Loss 2.1753\n",
            "Epoch 3 Batch 400 Loss 2.2855\n",
            "Epoch 3 Batch 500 Loss 2.2494\n",
            "Epoch 3 Batch 600 Loss 2.1836\n",
            "Epoch 3 Batch 700 Loss 2.2067\n",
            "Epoch 3 Batch 800 Loss 1.9730\n",
            "Epoch 3 Batch 900 Loss 2.1916\n",
            "Epoch 3 Loss 2.2013\n",
            "Time taken for 1 epoch 578.4358859062195 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.7499\n",
            "Epoch 4 Batch 100 Loss 1.8626\n",
            "Epoch 4 Batch 200 Loss 1.8934\n",
            "Epoch 4 Batch 300 Loss 1.7820\n",
            "Epoch 4 Batch 400 Loss 1.8212\n",
            "Epoch 4 Batch 500 Loss 1.9931\n",
            "Epoch 4 Batch 600 Loss 1.8505\n",
            "Epoch 4 Batch 700 Loss 1.8176\n",
            "Epoch 4 Batch 800 Loss 1.9500\n",
            "Epoch 4 Batch 900 Loss 1.7970\n",
            "Epoch 4 Loss 1.8309\n",
            "Time taken for 1 epoch 580.4336636066437 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.4809\n",
            "Epoch 5 Batch 100 Loss 1.4601\n",
            "Epoch 5 Batch 200 Loss 1.4201\n",
            "Epoch 5 Batch 300 Loss 1.4094\n",
            "Epoch 5 Batch 400 Loss 1.4556\n",
            "Epoch 5 Batch 500 Loss 1.5137\n",
            "Epoch 5 Batch 600 Loss 1.5782\n",
            "Epoch 5 Batch 700 Loss 1.5236\n",
            "Epoch 5 Batch 800 Loss 1.5287\n",
            "Epoch 5 Batch 900 Loss 1.3571\n",
            "Epoch 5 Loss 1.4877\n",
            "Time taken for 1 epoch 576.1011612415314 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.2247\n",
            "Epoch 6 Batch 100 Loss 1.1965\n",
            "Epoch 6 Batch 200 Loss 1.1391\n",
            "Epoch 6 Batch 300 Loss 1.1355\n",
            "Epoch 6 Batch 400 Loss 1.1239\n",
            "Epoch 6 Batch 500 Loss 1.1657\n",
            "Epoch 6 Batch 600 Loss 1.2716\n",
            "Epoch 6 Batch 700 Loss 1.2234\n",
            "Epoch 6 Batch 800 Loss 1.2745\n",
            "Epoch 6 Batch 900 Loss 1.2677\n",
            "Epoch 6 Loss 1.1814\n",
            "Time taken for 1 epoch 576.0232610702515 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxucQn9Uw2EP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0dfbee85-3b70-4e43-908f-40459909f5f0"
      },
      "source": [
        "preprocess_sentences_kor(['왜 안돼는 겁니까?'])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> ? 니까 겁 돼는 안 왜 <end>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdUHKPZYxRSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tSAOrv4zs2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentences_kor(sentence)[0]\n",
        "  inputs = inp_lang.texts_to_sequences([sentence])[0]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "\n",
        "\n",
        "    # the predicted ID is fed back into the mode\n",
        "\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13ggdBQ1zsbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBSGJsCpzxur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWgmbPwozxnM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "3082b3f0-c6f1-40ae-bfb7-bc2bd3fa8eb6"
      },
      "source": [
        "translate('hi')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-4ee2b8f266d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-bd54cd790ad2>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted translation: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-bc651235c027>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentences_kor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minp_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n\u001b[1;32m      8\u001b[0m                                                          \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-bc651235c027>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentences_kor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minp_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n\u001b[1;32m      8\u001b[0m                                                          \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'h'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP-TQmPAqhLB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d24054af-87c8-475e-ab85-ea6208d49234"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbf62d6dcc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1mDFK7Gz0Ur",
        "colab_type": "text"
      },
      "source": [
        "## BLUE SCORE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd9e17YKzand",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b8449db5-aba3-4689-e3ba-925d3898e806"
      },
      "source": [
        "a,_,_ = evaluate([test_kor.values[3]])\n",
        "a"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'i listened to music music in my house. <end> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNQbCl7J-Nlo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6cbc4b27-238b-4120-f86e-0187e5993f9d"
      },
      "source": [
        "test_kor.values[1]"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'이 프로그램은 준비하기 위한 과정으로 좋은 출발점이 될 것이에요.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quIU4Bl5762B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4b9a7f17-ed75-4f93-beed-7f38badc1c60"
      },
      "source": [
        "result,_,_ = evaluate([test_kor.values[1]])\n",
        "result"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'this program will be a good result for the stage. '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXPGK3PE3IqZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "outputId": "27ea77ff-51ac-40e5-eeed-ca3298d4f153"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "score_list = []\n",
        "for i in range(len(test_kor.values[:20])):\n",
        "    \n",
        "    reference = [test_en.values[i].split()]\n",
        "    result,_,_= evaluate([test_kor.values[i]])\n",
        "    candidate = result.capitalize().split()\n",
        "    score = sentence_bleu(reference, candidate, weights=(0.33,0.33,0.33), smoothing_function=SmoothingFunction().method1)\n",
        "    score_list.append(score)\n",
        "    print('실제값 : {}\\n예측값 : {}'.format(reference, candidate))\n",
        "\n",
        "score = sum(score_list)/len(score_list)\n",
        "print('score는 {}'.format(score))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "실제값 : [['Could', 'we', 'check', 'in', 'advance', 'and', 'prepare', 'that?']]\n",
            "예측값 : ['Could', 'we', 'check', 'our', 'check', 'and', 'check', 'out', 'this', 'coupon?']\n",
            "실제값 : [['This', 'program', 'would', 'be', 'a', 'good', 'starting', 'point', 'to', 'be', 'prepared.']]\n",
            "예측값 : ['This', 'program', 'will', 'be', 'a', 'good', 'result', 'for', 'the', 'stage.']\n",
            "실제값 : [['The', 'computer', 'I', 'ordered', 'on', 'the', 'internet', 'arrived', 'today.']]\n",
            "예측값 : ['I', 'ordered', 'arrived', 'in', 'the', 'internet', 'that', 'i', 'ordered', 'today.']\n",
            "실제값 : [['I', 'listened', 'to', 'classic', 'music', 'at', 'our', 'home.']]\n",
            "예측값 : ['I', 'listened', 'to', 'music', 'music', 'in', 'my', 'house.']\n",
            "실제값 : [['As', 'a', 'matter', 'of', 'fact,', \"I'm\", 'from', 'there,', 'too.']]\n",
            "예측값 : ['Even', 'if', 'i', 'told', 'me', 'whenever', 'i', 'wanted', 'to', 'see', 'it.']\n",
            "실제값 : [['Coeducational', 'schools', 'are', 'becoming', 'common.']]\n",
            "예측값 : ['The', 'school', 'is', 'being', 'born', 'as', 'a', 'prison', 'in', 'high', 'school.']\n",
            "실제값 : [['I', 'forgot', 'to', 'put', 'my', 'student', 'number', 'on', 'my', 'name.']]\n",
            "예측값 : ['I', 'misunderstood', 'the', 'story', 'of', 'the', 'name', 'of', 'the', 'name.']\n",
            "실제값 : [['He', 'has', 'a', 'knack', 'for', 'sports', 'and', 'is', 'good', 'at', 'everything.']]\n",
            "예측값 : ['I', 'pray', 'revivals', 'that', 'you', 'are', 'doing', 'yoga.']\n",
            "실제값 : [['Our', 'children', 'went', 'on', 'a', 'church', 'retreat.']]\n",
            "예측값 : ['Our', 'daughter', 'went', 'to', 'the', 'church', 'buffet', 'in', 'the', 'church.']\n",
            "실제값 : [['People', 'who', 'prefer', 'domestic', 'please', 'raise', 'your', 'hand.']]\n",
            "예측값 : ['People', 'who', 'make', 'people', 'supporting', 'other', 'traditional', 'foods.']\n",
            "실제값 : [['Korea', 'has', 'many', 'various', 'events', 'because', 'of', 'the', 'local', 'elections.']]\n",
            "예측값 : ['There', 'are', 'many', 'things', 'because', 'of', 'korea', 'becuase', 'of', 'south', 'korea.']\n",
            "실제값 : [['If', 'you', 'want', 'to', 'stay', 'at', 'my', 'house,', 'you', 'should', 'worship', 'God.']]\n",
            "예측값 : ['We', 'need', 'to', 'get', 'ready', 'to', 'the', 'rest', 'at', 'home.']\n",
            "실제값 : [['The', 'hot', 'teacup', 'which', 'holds', 'ink', 'on', 'coaster', 'can', 'see', 'the', 'time.']]\n",
            "예측값 : ['It', 'is', 'a', 'rough', 'to', 'wash', 'the', 'time', 'is', 'free.']\n",
            "실제값 : [['It', 'is', 'likely', 'that', 'I', 'would', 'be', 'going', 'to', 'the', 'library', 'a', 'lot.']]\n",
            "예측값 : ['I', 'think', 'i', 'can', 'go', 'to', 'the', 'library', 'to', 'go', 'home.']\n",
            "실제값 : [['I', 'was', 'so', 'foolish', 'to', 'think', 'that', 'you', 'were', 'being', 'honest.']]\n",
            "예측값 : ['I', 'thought', 'you', 'are', 'you', 'and', 'humility.']\n",
            "실제값 : [['The', 'second', 'thing', 'that', \"I'm\", 'interested', 'in', 'is', 'the', 'universe.']]\n",
            "예측값 : ['And', 'my', 'major', 'is', 'a', 'space', 'of', 'my', 'leadership.']\n",
            "실제값 : [['He', 'said', 'that', 'he', 'thought', 'I', 'would', 'come.']]\n",
            "예측값 : ['He', 'said', 'he', 'will', 'be', 'thought', 'about', 'it.']\n",
            "실제값 : [['I', 'want', 'to', 'work', 'at', 'a', 'hotel', 'front', 'desk.']]\n",
            "예측값 : ['I', 'want', 'to', 'work', 'at', 'the', 'hotel', 'in', 'the', 'hotel.']\n",
            "실제값 : [['First', 'nothing', 'appeared', 'to', 'be', 'a', 'problem.']]\n",
            "예측값 : ['That', 'was', 'not', 'the', 'reason', 'for', 'the', 'first', 'time.']\n",
            "실제값 : [['ive', 'always', 'had', 'the', 'Cup', 'Ramen', 'here.']]\n",
            "예측값 : ['I', 'ate', 'a', 'lot', 'of', 'the', 'place', 'where', 'i', 'am', 'hungry.']\n",
            "score는 0.09900054145922019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khtI17kcIyJE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "outputId": "88916597-5c34-43c3-fa35-0897daefe794"
      },
      "source": [
        "!pip install --upgrade bleu"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bleu\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/df/4fd9bfe6dc240a1760f8e95ca41aa31382e328e31de45145f528dab5c7f8/bleu-0.3.tar.gz\n",
            "Collecting efficiency\n",
            "  Downloading https://files.pythonhosted.org/packages/aa/9b/805e6da20cafbce2e2d08d92dce075d5cbb6316ecd473f4fe4f6373fb580/efficiency-0.4.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: spacy in /usr/local/lib/python3.6/dist-packages (from efficiency->bleu) (2.2.4)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu) (7.4.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu) (49.2.0)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->efficiency->bleu) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->efficiency->bleu) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->efficiency->bleu) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->efficiency->bleu) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->efficiency->bleu) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->efficiency->bleu) (3.1.0)\n",
            "Building wheels for collected packages: bleu, efficiency\n",
            "  Building wheel for bleu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bleu: filename=bleu-0.3-cp36-none-any.whl size=5801 sha256=1604ea6611d58cfb40100b53b9e3f2938525b66e930e2e498bdd3b93433beae0\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/95/e7/cb43a1c509c38fedbee6223963e34a51a94d8991f3b3e1888e\n",
            "  Building wheel for efficiency (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficiency: filename=efficiency-0.4-cp36-none-any.whl size=19617 sha256=f3c8cddc3664ab6b7ebf0498ddae77b9b42c85cf38662bc9f071c3b905532710\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/67/60/11693f94754d75dabfadb120c7ba380e7ef2aed8d3dccd00bf\n",
            "Successfully built bleu efficiency\n",
            "Installing collected packages: efficiency, bleu\n",
            "Successfully installed bleu-0.3 efficiency-0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saExsHAWI2Z3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "outputId": "e54597e9-0e6c-479b-e01e-dc3957ca12ea"
      },
      "source": [
        "pip install --upgrade git+git://github.com/zhijing-jin/bleu.git"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/zhijing-jin/bleu.git\n",
            "  Cloning git://github.com/zhijing-jin/bleu.git to /tmp/pip-req-build-awncxr3a\n",
            "  Running command git clone -q git://github.com/zhijing-jin/bleu.git /tmp/pip-req-build-awncxr3a\n",
            "Requirement already satisfied, skipping upgrade: efficiency in /usr/local/lib/python3.6/dist-packages (from bleu==0.3) (0.4)\n",
            "Requirement already satisfied, skipping upgrade: spacy in /usr/local/lib/python3.6/dist-packages (from efficiency->bleu==0.3) (2.2.4)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu==0.3) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu==0.3) (7.4.0)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu==0.3) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu==0.3) (49.2.0)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu==0.3) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu==0.3) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu==0.3) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu==0.3) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu==0.3) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu==0.3) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu==0.3) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu==0.3) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy->efficiency->bleu==0.3) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->efficiency->bleu==0.3) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->efficiency->bleu==0.3) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->efficiency->bleu==0.3) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->efficiency->bleu==0.3) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->efficiency->bleu==0.3) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->efficiency->bleu==0.3) (3.1.0)\n",
            "Building wheels for collected packages: bleu\n",
            "  Building wheel for bleu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bleu: filename=bleu-0.3-cp36-none-any.whl size=6773 sha256=6dbb68b02b5867dfec68baddb51a78b12e24919d464df591e7af56300ae4c351\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wl3pk52v/wheels/2b/79/93/d67db42308d30f8f11610565063bbf30c0f1ce08a23dfb95d8\n",
            "Successfully built bleu\n",
            "Installing collected packages: bleu\n",
            "  Found existing installation: bleu 0.3\n",
            "    Uninstalling bleu-0.3:\n",
            "      Successfully uninstalled bleu-0.3\n",
            "Successfully installed bleu-0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lRJZvX4JVKE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "32c2d13c-b892-4a50-f8f0-1d451d6837fb"
      },
      "source": [
        "result,_,_ = evaluate([test_kor.values[1]])\n",
        "result"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'this program will be a good result for the stage. '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMnjfqW2JRoM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2192ab45-6d17-4a82-aa73-b9841bbba0b9"
      },
      "source": [
        "from bleu import list_bleu\n",
        "ref = ['wow , this dog is huge .']\n",
        "ref1 = ['This cat is white .',\n",
        "             'wow , this is a huge dog .']\n",
        "hyp = ['wowww , the dog is huge !']\n",
        "hyp1 = [\"it 's a white kitten .\",\n",
        "             'wow , this dog is huge !']\n",
        "list_bleu([ref], hyp)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9cAqquILlqy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "829dfc06-ea3f-44f8-8ef7-59c74172cbd8"
      },
      "source": [
        "test_kor.values[:10]"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['그것을 우리가 미리 확인하고 준비할 수 있을까요?',\n",
              "       '이 프로그램은 준비하기 위한 과정으로 좋은 출발점이 될 것이에요.',\n",
              "       '오늘 내가 인터넷으로 주문한 컴퓨터가 도착했어요.', '우리집에서 클래식 음악을 들었어.',\n",
              "       '사실대로 말하면, 저도 거기에서 왔어요.', '남녀 공학인 학교가 일반화되고 있어요.',\n",
              "       '이름에 학번을 넣는 것을 잊어 버렸어요.', '운동신경이 좋아 모든 운동을잘해요.',\n",
              "       '우리 애 들은 교회 수련회 갔어.', '국내를 선호하는 사람은 손을 들어주세요.'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOjVM6eiMZZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_sentence = []\n",
        "for i in test_kor.values[:300]:\n",
        "    a,_,_= evaluate([i])\n",
        "    predicted_sentence.append(a.capitalize())"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xleYNri4I8SW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d6e922fa-113d-4dc0-8a74-9f7a2d8bf710"
      },
      "source": [
        "from bleu import list_bleu\n",
        "print(test_en.values[0], predicted_sentence[0])\n",
        "score = list_bleu([test_en.values[:300]], predicted_sentence[:300])\n",
        "print(score)\n"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Could we check in advance and prepare that? Could we check our check and check out this coupon? \n",
            "7.88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5jhnUt_cxbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trans(lang, sentences):\n",
        "    result_list=[]\n",
        "    for line in sentences:\n",
        "        result = ''\n",
        "        for idx in line:\n",
        "            if idx==1:continue\n",
        "            result += lang.index_word[idx] + ' '\n",
        "            if lang.index_word[idx] == '<end>':\n",
        "                result_list.append(result)\n",
        "                break\n",
        "\n",
        "        \n",
        "    return result_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSnGuFRTd0KT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = trans(targ_lang, target_tensor_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu5_CirgyS9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = trans(inp_lang, input_tensor_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}